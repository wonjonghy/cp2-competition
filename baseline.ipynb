{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WONJONGHYEON\\anaconda3\\envs\\CP2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':10,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':256,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 불러오기 및 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip open.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')\n",
    "df_copy=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv('test.csv')\n",
    "test_copy=test.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 벡터화 및 레이블링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_type=df[['문장', '유형']]\n",
    "df_polar=df[['문장', '극성']]\n",
    "df_tense=df[['문장', '시제']]\n",
    "df_certainty=df[['문장', '확실성']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding(df, label):\n",
    "  encoder=LabelEncoder()\n",
    "  df[label]=encoder.fit_transform(df[label])\n",
    "  print({category: label for category, label in zip(encoder.classes_, encoder.transform(encoder.classes_))})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'대화형': 0, '사실형': 1, '예측형': 2, '추론형': 3}\n",
      "{'긍정': 0, '미정': 1, '부정': 2}\n",
      "{'과거': 0, '미래': 1, '현재': 2}\n",
      "{'불확실': 0, '확실': 1}\n"
     ]
    }
   ],
   "source": [
    "encoding(df_type, '유형')\n",
    "encoding(df_polar, '극성')\n",
    "encoding(df_tense, '시제')\n",
    "encoding(df_certainty, '확실성')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type, val_type, train_type_label, val_type_label = train_test_split(df_type['문장'], df_type['유형'], test_size=0.2, random_state=CFG['SEED'], stratify=df_type['유형'])\n",
    "train_polar, val_polar, train_polar_label, val_polar_label = train_test_split(df_polar['문장'], df_polar['극성'], test_size=0.2, random_state=CFG['SEED'], stratify=df_polar['극성'])\n",
    "train_tense, val_tense, train_tense_label, val_tense_label = train_test_split(df_tense['문장'], df_tense['시제'], test_size=0.2, random_state=CFG['SEED'], stratify=df_tense['시제'])\n",
    "train_certainty, val_certainty, train_certainty_label, val_certainty_label = train_test_split(df_certainty['문장'], df_certainty['확실성'], test_size=0.2, random_state=CFG['SEED'], stratify=df_certainty['확실성'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_label.reset_index(drop=True, inplace=True)\n",
    "train_type.reset_index(drop=True, inplace=True)\n",
    "val_type_label.reset_index(drop=True, inplace=True)\n",
    "val_type.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_polar_label.reset_index(drop=True, inplace=True)\n",
    "train_polar.reset_index(drop=True, inplace=True)\n",
    "val_polar_label.reset_index(drop=True, inplace=True)\n",
    "val_polar.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_tense_label.reset_index(drop=True, inplace=True)\n",
    "train_tense.reset_index(drop=True, inplace=True)\n",
    "val_tense_label.reset_index(drop=True, inplace=True)\n",
    "val_tense.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_certainty_label.reset_index(drop=True, inplace=True)\n",
    "train_certainty.reset_index(drop=True, inplace=True)\n",
    "val_certainty_label.reset_index(drop=True, inplace=True)\n",
    "val_certainty.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 4, analyzer = 'word', ngram_range=(1, 2))\n",
    "vectorizer.fit(np.array(df[\"문장\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_type_vec = vectorizer.transform(train_type)\n",
    "val_type_vec = vectorizer.transform(val_type)\n",
    "\n",
    "train_polar_vec = vectorizer.transform(train_polar)\n",
    "val_polar_vec = vectorizer.transform(val_polar)\n",
    "\n",
    "train_tense_vec = vectorizer.transform(train_tense)\n",
    "val_tense_vec = vectorizer.transform(val_tense)\n",
    "\n",
    "train_certainty_vec = vectorizer.transform(train_certainty)\n",
    "val_certainty_vec = vectorizer.transform(val_certainty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13232, 12028) (3309, 12028) (7090, 12028)\n"
     ]
    }
   ],
   "source": [
    "test_vec = vectorizer.transform(test[\"문장\"])\n",
    "\n",
    "print(train_type_vec.shape, val_type_vec.shape, test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, st_vec, st_labels):\n",
    "        self.st_vec = st_vec\n",
    "        self.st_labels = st_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        st_vector = torch.FloatTensor(self.st_vec[index].toarray()).squeeze(0)\n",
    "        if self.st_labels is not None:\n",
    "            st_label = self.st_labels[index]\n",
    "            return st_vector, st_label\n",
    "        else:\n",
    "            return st_vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.st_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_type_loader))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_dataset = CustomDataset(train_type_vec, train_type_label)\n",
    "train_type_loader = DataLoader(train_type_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_type_dataset = CustomDataset(val_type_vec, val_type_label)\n",
    "val_type_loader = DataLoader(val_type_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 베이스모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim=9351):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=2),\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x)\n",
    "\n",
    "        return type_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[108], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m BaseModel(\u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_type_loader))[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device))\n",
      "Cell \u001b[1;32mIn[104], line 5\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[1;34m(self, input_dim)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_dim\u001b[39m=\u001b[39m\u001b[39m9351\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[39msuper\u001b[39m(BaseModel, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m      4\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_extract \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[1;32m----> 5\u001b[0m         nn\u001b[39m.\u001b[39;49mLinear(in_features\u001b[39m=\u001b[39;49minput_dim, out_features\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m),\n\u001b[0;32m      6\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm1d(\u001b[39m1024\u001b[39m),\n\u001b[0;32m      7\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(),\n\u001b[0;32m      8\u001b[0m         nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m),\n\u001b[0;32m      9\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm1d(\u001b[39m1024\u001b[39m),\n\u001b[0;32m     10\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(),\n\u001b[0;32m     11\u001b[0m         nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m1024\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m),\n\u001b[0;32m     12\u001b[0m         nn\u001b[39m.\u001b[39mBatchNorm1d(\u001b[39m512\u001b[39m),\n\u001b[0;32m     13\u001b[0m         nn\u001b[39m.\u001b[39mLeakyReLU(),\n\u001b[0;32m     14\u001b[0m     )\n\u001b[0;32m     15\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_classifier \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     16\u001b[0m         nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m),\n\u001b[0;32m     17\u001b[0m         nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m),\n\u001b[0;32m     18\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolarity_classifier \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m     20\u001b[0m         nn\u001b[39m.\u001b[39mDropout(p\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m),\n\u001b[0;32m     21\u001b[0m         nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m512\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m),\n\u001b[0;32m     22\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\WONJONGHYEON\\anaconda3\\envs\\CP2\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type Tensor at pos 2"
     ]
    }
   ],
   "source": [
    "BaseModel(next(iter(train_type_loader))[0].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device, loader):\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss().to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "    }\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for sentence, label in tqdm(iter(loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logit = model(sentence)\n",
    "            \n",
    "            loss = criterion['type'](type_logit, type_label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_type_f1 = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] 유형 F1 : [{val_type_f1:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    type_labels, polarity_labels, tense_labels, certainty_labels = [], [], [], []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence, label in tqdm(iter(val_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            label = type_label.to(device)\n",
    "\n",
    "            \n",
    "            logit = model(sentence)\n",
    "            \n",
    "            loss = criterion['type'](type_logit, type_label)\n",
    "\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            type_labels += type_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "    type_f1 = f1_score(type_labels, type_preds, average='weighted')\n",
    "\n",
    "    \n",
    "    return type_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train() missing 1 required positional argument: 'loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[103], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(params \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m CFG[\u001b[39m\"\u001b[39m\u001b[39mLEARNING_RATE\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m scheduler \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m'\u001b[39m, factor\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,threshold_mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mabs\u001b[39m\u001b[39m'\u001b[39m,min_lr\u001b[39m=\u001b[39m\u001b[39m1e-8\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m----> 6\u001b[0m infer_model \u001b[39m=\u001b[39m train(model, optimizer, train_loader, val_loader, scheduler, device)\n",
      "\u001b[1;31mTypeError\u001b[0m: train() missing 1 required positional argument: 'loader'"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_vec, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_loader):\n",
    "            sentence = sentence.to(device)\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            polarity_preds += polarity_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            tense_preds += tense_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            certainty_preds += certainty_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            \n",
    "    return type_preds, polarity_preds, tense_preds, certainty_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_preds, polarity_preds, tense_preds, certainty_preds = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_preds = type_le.inverse_transform(type_preds)\n",
    "polarity_preds = polarity_le.inverse_transform(polarity_preds)\n",
    "tense_preds = tense_le.inverse_transform(tense_preds)\n",
    "certainty_preds = certainty_le.inverse_transform(certainty_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for type_pred, polarity_pred, tense_pred, certainty_pred in zip(type_preds, polarity_preds, tense_preds, certainty_preds):\n",
    "    predictions.append(type_pred+'-'+polarity_pred+'-'+tense_pred+'-'+certainty_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e5bac9dc713ed5e7b0e2455bebe074a3a25a24fa4d9650e1e42f8c8e58e6ce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
