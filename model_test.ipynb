{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\WONJONGHYEON\\anaconda3\\envs\\CP2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import numpy as np\n",
    "from typing import Optional, Sequence\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, f1_score\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.nn import functional as F\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification, AutoTokenizer, EarlyStoppingCallback, AutoModel, AutoConfig\n",
    "\n",
    "import gc\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "kind_aug=pd.read_csv(r'C:\\Users\\WONJONGHYEON\\OneDrive\\바탕 화면\\cp2-competition\\augment_csv\\df_kind_aug.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>유형</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>박 회장은 1년 1월 경부고속도로 대전공구 시험계장으로 발령을 받는다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>국내 게임업계에서 신종 코로나바이러스 감염증코로나1 환자가 발생했다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>코로나1는 인류에게 많은 변화를 요구하고 있다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>또 자유공정혁신연대를 1대 정책 기조로 잡고 경제운용 중심축을 정부에서 민간으로 바...</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>젊은 층 외지인 수요가 몰리면서 집값이 하늘을 모르고 치솟았다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>이날 한밤중인 한때 접속 대기자가 넘겼고 오전 오후 1만1여명이 몰렸다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>이날 오전 한때 넘겼고 대기자가 1만명을 접속 한밤중인 오후 1만1여명이 몰렸다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18721</th>\n",
       "      <td>이날 오전 오후 접속 대기자가 넘겼고 한밤중인 한때 1시에도 1만1여명이 몰렸다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18722</th>\n",
       "      <td>이날 오전 한때 접속 대기자가 1만명을 1만1여명이 한밤중인 오후 넘겼고 몰렸다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18723</th>\n",
       "      <td>이날 오전 1만명을 접속 대기자가 한때 넘겼고 한밤중인 오후 몰렸다</td>\n",
       "      <td>사실형</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18724 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장   유형\n",
       "0                 박 회장은 1년 1월 경부고속도로 대전공구 시험계장으로 발령을 받는다  사실형\n",
       "1                  국내 게임업계에서 신종 코로나바이러스 감염증코로나1 환자가 발생했다  사실형\n",
       "2                              코로나1는 인류에게 많은 변화를 요구하고 있다  사실형\n",
       "3      또 자유공정혁신연대를 1대 정책 기조로 잡고 경제운용 중심축을 정부에서 민간으로 바...  사실형\n",
       "4                     젊은 층 외지인 수요가 몰리면서 집값이 하늘을 모르고 치솟았다  사실형\n",
       "...                                                  ...  ...\n",
       "18719            이날 한밤중인 한때 접속 대기자가 넘겼고 오전 오후 1만1여명이 몰렸다  사실형\n",
       "18720       이날 오전 한때 넘겼고 대기자가 1만명을 접속 한밤중인 오후 1만1여명이 몰렸다  사실형\n",
       "18721       이날 오전 오후 접속 대기자가 넘겼고 한밤중인 한때 1시에도 1만1여명이 몰렸다  사실형\n",
       "18722       이날 오전 한때 접속 대기자가 1만명을 1만1여명이 한밤중인 오후 넘겼고 몰렸다  사실형\n",
       "18723              이날 오전 1만명을 접속 대기자가 한때 넘겼고 한밤중인 오후 몰렸다  사실형\n",
       "\n",
       "[18724 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kind_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "model_path = \"monologg/kobigbird-bert-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = kind_aug['문장'].str.len().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kind_aug['문장'].str.len().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "496"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        if self.labels:\n",
    "            st_type = self.labels['type'][idx]\n",
    "            st_polarity = self.labels['polarity'][idx]\n",
    "            st_tense = self.labels['tense'][idx]\n",
    "            st_certainty = self.labels['certainty'][idx]\n",
    "            item[\"labels\"] = torch.tensor(st_type), torch.tensor(st_polarity), torch.tensor(st_tense), torch.tensor(st_certainty)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 870/870 [00:00<00:00, 79.1kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "print(config.num_hidden_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        if model_path == 'monologg/kobigbird-bert-base':\n",
    "            config.attention_type = \"original_full\"\n",
    "\n",
    "        self.base_model = AutoModel.from_pretrained(model_path, config=config)\n",
    "        \n",
    "        self.out = 768\n",
    "        # self.linear = nn.Linear(768, 768//2)\n",
    "\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features=self.out, out_features=2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, labels=None, token_type_ids=None):\n",
    "        x = self.base_model(input_ids=input_ids, attention_mask=attention_mask)[0]\n",
    "        # x = self.linear(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x[:,0,:].view(-1,self.out))\n",
    "        polarity_output = self.polarity_classifier(x[:,0,:].view(-1,self.out))\n",
    "        tense_output = self.tense_classifier(x[:,0,:].view(-1,self.out))\n",
    "        certainty_output = self.certainty_classifier(x[:,0,:].view(-1,self.out))\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trainer arguments\n",
    "lr = 1e-4\n",
    "stop = 3\n",
    "epoch = 1000\n",
    "batch = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    \"\"\" Focal Loss, as described in https://arxiv.org/abs/1708.02002.\n",
    "    It is essentially an enhancement to cross entropy loss and is\n",
    "    useful for classification tasks when there is a large class imbalance.\n",
    "    x is expected to contain raw, unnormalized scores for each class.\n",
    "    y is expected to contain class labels.\n",
    "    Shape:\n",
    "        - x: (batch_size, C) or (batch_size, C, d1, d2, ..., dK), K > 0.\n",
    "        - y: (batch_size,) or (batch_size, d1, d2, ..., dK), K > 0.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 alpha: Optional[Tensor] = None,\n",
    "                 gamma: float = 0.,\n",
    "                 reduction: str = 'mean',\n",
    "                 ignore_index: int = -100):\n",
    "        \"\"\"Constructor.\n",
    "        Args:\n",
    "            alpha (Tensor, optional): Weights for each class. Defaults to None.\n",
    "            gamma (float, optional): A constant, as described in the paper.\n",
    "                Defaults to 0.\n",
    "            reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "                Defaults to 'mean'.\n",
    "            ignore_index (int, optional): class label to ignore.\n",
    "                Defaults to -100.\n",
    "        \"\"\"\n",
    "        if reduction not in ('mean', 'sum', 'none'):\n",
    "            raise ValueError(\n",
    "                'Reduction must be one of: \"mean\", \"sum\", \"none\".')\n",
    "\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.ignore_index = ignore_index\n",
    "        self.reduction = reduction\n",
    "\n",
    "        self.nll_loss = nn.NLLLoss(\n",
    "            weight=alpha, reduction='none', ignore_index=ignore_index)\n",
    "\n",
    " def __repr__(self):\n",
    "        arg_keys = ['alpha', 'gamma', 'ignore_index', 'reduction']\n",
    "        arg_vals = [self.__dict__[k] for k in arg_keys]\n",
    "        arg_strs = [f'{k}={v!r}' for k, v in zip(arg_keys, arg_vals)]\n",
    "        arg_str = ', '.join(arg_strs)\n",
    "        return f'{type(self).__name__}({arg_str})'\n",
    "\n",
    "    def forward(self, x: Tensor, y: Tensor) -> Tensor:\n",
    "        if x.ndim > 2:\n",
    "            # (N, C, d1, d2, ..., dK) --> (N * d1 * ... * dK, C)\n",
    "            c = x.shape[1]\n",
    "            x = x.permute(0, *range(2, x.ndim), 1).reshape(-1, c)\n",
    "            # (N, d1, d2, ..., dK) --> (N * d1 * ... * dK,)\n",
    "            y = y.view(-1)\n",
    "\n",
    "        unignored_mask = y != self.ignore_index\n",
    "        y = y[unignored_mask]\n",
    "        if len(y) == 0:\n",
    "            return torch.tensor(0.)\n",
    "        x = x[unignored_mask]\n",
    "\n",
    "        # compute weighted cross entropy term: -alpha * log(pt)\n",
    "        # (alpha is already part of self.nll_loss)\n",
    "        log_p = F.log_softmax(x, dim=-1)\n",
    "        ce = self.nll_loss(log_p, y)\n",
    "\n",
    "        # get true class column from each row\n",
    "        all_rows = torch.arange(len(x))\n",
    "        log_pt = log_p[all_rows, y]\n",
    "\n",
    "        # compute focal term: (1 - pt)^gamma\n",
    "        pt = log_pt.exp()\n",
    "        focal_term = (1 - pt)**self.gamma\n",
    "\n",
    "        # the full loss: -alpha * ((1 - pt)^gamma) * log(pt)\n",
    "        loss = focal_term * ce\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "\n",
    "        return loss\n",
    "\n",
    "def focal_loss(alpha: Optional[Sequence] = None,\n",
    "               gamma: float = 0.,\n",
    "               reduction: str = 'mean',\n",
    "               ignore_index: int = -100,\n",
    "               device='cpu',\n",
    "               dtype=torch.float32) -> FocalLoss:\n",
    "    \"\"\"Factory function for FocalLoss.\n",
    "    Args:\n",
    "        alpha (Sequence, optional): Weights for each class. Will be converted\n",
    "            to a Tensor if not None. Defaults to None.\n",
    "        gamma (float, optional): A constant, as described in the paper.\n",
    "            Defaults to 0.\n",
    "        reduction (str, optional): 'mean', 'sum' or 'none'.\n",
    "            Defaults to 'mean'.\n",
    "        ignore_index (int, optional): class label to ignore.\n",
    "            Defaults to -100.\n",
    "        device (str, optional): Device to move alpha to. Defaults to 'cpu'.\n",
    "        dtype (torch.dtype, optional): dtype to cast alpha to.\n",
    "            Defaults to torch.float32.\n",
    "    Returns:\n",
    "        A FocalLoss object\n",
    "    \"\"\"\n",
    "    if alpha is not None:\n",
    "        if not isinstance(alpha, Tensor):\n",
    "            alpha = torch.tensor(alpha)\n",
    "        alpha = alpha.to(device=device, dtype=dtype)\n",
    "\n",
    "    fl = FocalLoss(\n",
    "        alpha=alpha,\n",
    "        gamma=gamma,\n",
    "        reduction=reduction,\n",
    "        ignore_index=ignore_index)\n",
    "    return fl\n",
    "        \n",
    "def compute_metrics(pred):\n",
    "    # label = [[cls1,cls2,...],]\n",
    "    # preds = n list\n",
    "    focal_loss = FocalLoss()\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions\n",
    "    f1 = []\n",
    "    focal = []\n",
    "    for i in range(4):\n",
    "        # focal.append(focal_loss(torch.tensor(preds[i], dtype=torch.float), torch.tensor(labels[::, i],dtype=torch.float)))\n",
    "        f1.append(f1_score(y_true = labels[::, i], y_pred = preds[i], average='weighted'))\n",
    "    return {\n",
    "        #'focal': sum(focal),\n",
    "        'f1-sum': sum(f1)/4\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineAnnealingWarmUpRestarts(_LRScheduler):\n",
    "    def __init__(self, optimizer, T_0, T_mult=1, eta_max=0.1, T_up=0, gamma=1., last_epoch=-1):\n",
    "        if T_0 <= 0 or not isinstance(T_0, int):\n",
    "            raise ValueError(\"Expected positive integer T_0, but got {}\".format(T_0))\n",
    "        if T_mult < 1 or not isinstance(T_mult, int):\n",
    "            raise ValueError(\"Expected integer T_mult >= 1, but got {}\".format(T_mult))\n",
    "        if T_up < 0 or not isinstance(T_up, int):\n",
    "            raise ValueError(\"Expected positive integer T_up, but got {}\".format(T_up))\n",
    "        self.T_0 = T_0\n",
    "        self.T_mult = T_mult\n",
    "        self.base_eta_max = eta_max\n",
    "        self.eta_max = eta_max\n",
    "        self.T_up = T_up\n",
    "        self.T_i = T_0\n",
    "        self.gamma = gamma\n",
    "        self.cycle = 0\n",
    "        self.T_cur = last_epoch\n",
    "        super(CosineAnnealingWarmUpRestarts, self).__init__(optimizer, last_epoch)\n",
    "    \n",
    "    def get_lr(self):\n",
    "        if self.T_cur == -1:\n",
    "            return self.base_lrs\n",
    "        elif self.T_cur < self.T_up:\n",
    "            return [(self.eta_max - base_lr)*self.T_cur / self.T_up + base_lr for base_lr in self.base_lrs]\n",
    "        else:\n",
    "            return [base_lr + (self.eta_max - base_lr) * (1 + math.cos(math.pi * (self.T_cur-self.T_up) / (self.T_i - self.T_up))) / 2\n",
    "                    for base_lr in self.base_lrs]\n",
    "\n",
    "    def step(self, epoch=None):\n",
    "        if epoch is None:\n",
    "            epoch = self.last_epoch + 1\n",
    "            self.T_cur = self.T_cur + 1\n",
    "            if self.T_cur >= self.T_i:\n",
    "                self.cycle += 1\n",
    "                self.T_cur = self.T_cur - self.T_i\n",
    "                self.T_i = (self.T_i - self.T_up) * self.T_mult + self.T_up\n",
    "        else:\n",
    "            if epoch >= self.T_0:\n",
    "                if self.T_mult == 1:\n",
    "                    self.T_cur = epoch % self.T_0\n",
    "                    self.cycle = epoch // self.T_0\n",
    "                else:\n",
    "                    n = int(math.log((epoch / self.T_0 * (self.T_mult - 1) + 1), self.T_mult))\n",
    "                    self.cycle = n\n",
    "                    self.T_cur = epoch - self.T_0 * (self.T_mult ** n - 1) / (self.T_mult - 1)\n",
    "                    self.T_i = self.T_0 * self.T_mult ** (n)\n",
    "            else:\n",
    "                self.T_i = self.T_0\n",
    "                self.T_cur = epoch\n",
    "                \n",
    "        self.eta_max = self.base_eta_max * (self.gamma**self.cycle)\n",
    "        self.last_epoch = math.floor(epoch)\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr = 0)\n",
    "# scheduler = CosineAnnealingWarmUpRestarts(optimizer, T_0=150, T_mult=1, eta_max=0.1,  T_up=10, gamma=0.5)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, \n",
    "#                                                                  T_mult=2, eta_min=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define trainer\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # forward pass\n",
    "        labels = inputs.pop(\"labels\").to(torch.int64)\n",
    "        \n",
    "        type_logit, polarity_logit, tense_logit, certainty_logit = model(**inputs)\n",
    "        \n",
    "        # # simple loss\n",
    "        # criterion = {\n",
    "        #     'type' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        #     'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "        # }\n",
    "        # loss = criterion['type'](type_logit, labels[::, 0]) + \\\n",
    "        #             criterion['polarity'](polarity_logit, labels[::, 1]) + \\\n",
    "        #             criterion['tense'](tense_logit,labels[::, 2]) + \\\n",
    "        #             criterion['certainty'](certainty_logit, labels[::, 3])\n",
    "        \n",
    "        # focal loss\n",
    "        criterion = {\n",
    "            'type' : FocalLoss().to(device),\n",
    "            'polarity' : FocalLoss().to(device),\n",
    "            'tense' : FocalLoss().to(device),\n",
    "            'certainty' : FocalLoss().to(device)\n",
    "        }\n",
    "        # labels = labels.type(torch.float).clone().detach()\n",
    "        loss = criterion['type'](type_logit, labels[::, 0]) + \\\n",
    "                    criterion['polarity'](polarity_logit, labels[::, 1]) + \\\n",
    "                    criterion['tense'](tense_logit, labels[::, 2]) + \\\n",
    "                    criterion['certainty'](certainty_logit, labels[::, 3])\n",
    "\n",
    "        outputs = None, \\\n",
    "                    torch.argmax(type_logit, dim = 1), \\\n",
    "                    torch.argmax(polarity_logit, dim = 1),\\\n",
    "                    torch.argmax(tense_logit, dim = 1),\\\n",
    "                    torch.argmax(certainty_logit, dim = 1)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "유형 = LabelEncoder()\n",
    "유형.fit(train['유형'])\n",
    "\n",
    "극성 = LabelEncoder()\n",
    "극성.fit(train['극성'])\n",
    "\n",
    "시제 = LabelEncoder()\n",
    "시제.fit(train['시제'])\n",
    "\n",
    "확실성 = LabelEncoder()\n",
    "확실성.fit(train['확실성'])\n",
    "\n",
    "def encoding(X_train, X_val):\n",
    "    X_train['유형'] = 유형.transform(X_train['유형'])\n",
    "    X_val['유형'] = 유형.transform(X_val['유형'])\n",
    "\n",
    "    X_train['극성'] = 극성.transform(X_train['극성'])\n",
    "    X_val['극성'] = 극성.transform(X_val['극성'])\n",
    "\n",
    "    X_train['시제'] = 시제.transform(X_train['시제'])\n",
    "    X_val['시제'] = 시제.transform(X_val['시제'])\n",
    "\n",
    "    X_train['확실성'] = 확실성.transform(X_train['확실성'])\n",
    "    X_val['확실성'] = 확실성.transform(X_val['확실성'])\n",
    "\n",
    "    train_labels = {\n",
    "        'type' : X_train['유형'].values,\n",
    "        'polarity' : X_train['극성'].values,\n",
    "        'tense' : X_train['시제'].values,\n",
    "        'certainty' : X_train['확실성'].values\n",
    "    }\n",
    "\n",
    "    val_labels = {\n",
    "        'type' : X_val['유형'].values,\n",
    "        'polarity' : X_val['극성'].values,\n",
    "        'tense' : X_val['시제'].values,\n",
    "        'certainty' : X_val['확실성'].values\n",
    "    }\n",
    "    return train_labels, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=AutoConfig.from_pretrained(model_path)\n",
    "config._name_or_path = 'kr.kim'\n",
    "print(f'hidden_layers : {config.num_hidden_layers}')\n",
    "# config.num_hidden_layers = 17\n",
    "\n",
    "kf = KFold(n_splits=5, random_state=42, shuffle=True)\n",
    "for i, (train_index, test_index) in enumerate(kf.split(train)):\n",
    "    print(f'Round {i}')\n",
    "    X_train, X_val = train.loc[train_index, :], train.loc[test_index, :]\n",
    "    train_labels, val_labels = encoding(X_train, X_val)\n",
    "    token_train, token_val = tokenizer(X_train.문장.tolist(), padding=True, truncation=True, max_length=length), tokenizer(X_val.문장.tolist(), padding=True, truncation=True, max_length=length)\n",
    "    train_dataset, val_dataset = CustomDataset(token_train, train_labels), CustomDataset(token_val, val_labels)\n",
    "    model = CustomModel()\n",
    "    model.to(device)\n",
    "    args = TrainingArguments(run_name = f'fold_{i}',\n",
    "                             output_dir= f\"fold_{i}\",                                   # 모델저장경로\n",
    "                             evaluation_strategy=\"steps\",                           # 모델의 평가를 언제 진행할지\n",
    "                             eval_steps=10,                                        # 500 스텝 마다 모델 평가\n",
    "                             logging_steps=10,\n",
    "                             per_device_train_batch_size=batch,                        # GPU에 학습데이터를 몇개씩 올려서 학습할지\n",
    "                             per_device_eval_batch_size=batch,                         # GPU에 학습데이터를 몇개씩 올려서 평가할지\n",
    "                             gradient_accumulation_steps=16,\n",
    "                             num_train_epochs=epoch,                                  # 전체 학습 진행 횟수\n",
    "                             learning_rate=lr,                                      # 학습률 정의 \n",
    "                             seed=42,                                                 \n",
    "                             load_best_model_at_end=True,                           # 평가기준 스코어가 좋은 모델만 저장할지 여부\n",
    "                             fp16=True,\n",
    "                             do_train=True,\n",
    "                             do_eval=True,\n",
    "                             save_steps=10,\n",
    "                             save_total_limit = 2,                                  # 저장할 모델의 갯수\n",
    "                             # metric_for_best_model\n",
    "                             # greater_is_better = True,\n",
    "    )\n",
    "    trainer = CustomTrainer(model=model,\n",
    "                            args=args,\n",
    "                            train_dataset=train_dataset,                      # 학습데이터\n",
    "                            eval_dataset=val_dataset,                        # validation 데이터\n",
    "                            compute_metrics=compute_metrics,                       # 모델 평가 방식\n",
    "                            callbacks=[EarlyStoppingCallback(early_stopping_patience=stop)],)\n",
    "    trainer.train()\n",
    "    del model\n",
    "    del trainer\n",
    "    gc.collect() # python 자원 관리 \n",
    "    torch.cuda.empty_cache() # gpu 자원관리   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recent_file(path):\n",
    "    file_name_and_time_lst = []\n",
    "    # 해당 경로에 있는 파일들의 생성시간을 함께 리스트로 넣어줌. \n",
    "    for f_name in os.listdir(f\"{path}\"):\n",
    "        written_time = os.path.getctime(f\"{path}/{f_name}\")\n",
    "        file_name_and_time_lst.append((f_name, written_time))\n",
    "    # 생성시간 역순으로 정렬하고, \n",
    "    sorted_file_lst = sorted(file_name_and_time_lst, key=lambda x: x[1], reverse=True)\n",
    "    # 가장 앞에 이는 놈을 넣어준다.\n",
    "    recent_file = sorted_file_lst[0]\n",
    "    recent_file_name = recent_file[0]\n",
    "    return f\"{path}/{recent_file_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect() # python 자원 관리 \n",
    "torch.cuda.empty_cache() # gpu 자원관리\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenized = tokenizer(test.문장.tolist(), padding=True, truncation=True, max_length=length, return_tensors=\"pt\")\n",
    "test_dataset = CustomDataset(tokenized, None)\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 512,   \n",
    "    dataloader_drop_last = False    \n",
    ")\n",
    "\n",
    "tmp = 0\n",
    "while os.path.isdir(f'fold_{tmp}'):\n",
    "    tmp += 1\n",
    "\n",
    "test_results = []\n",
    "for i in range(tmp):\n",
    "    print(f'Round {i}')\n",
    "    # model = AutoModel.from_pretrained(recent_file('custom_model'), config=config)\n",
    "    model = CustomModel().to(device)\n",
    "    model.load_state_dict(torch.load(f\"{recent_file(f'fold_{i}')}/pytorch_model.bin\"))\n",
    "    trainer = CustomTrainer(\n",
    "                  model = model, \n",
    "                  args = test_args, \n",
    "                  compute_metrics = compute_metrics)\n",
    "    test_results.append(trainer.predict(test_dataset))\n",
    "    gc.collect() # python 자원 관리 \n",
    "    torch.cuda.empty_cache() # gpu 자원관리\n",
    "    del model\n",
    "    del trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('sample_submission.csv')\n",
    "sub['label'] = test['label']\n",
    "tmp = 0\n",
    "while os.path.exists(f'제출{tmp}.csv'):\n",
    "    tmp += 1\n",
    "sub.to_csv(f'제출{tmp}.csv', index=False, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CP2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
